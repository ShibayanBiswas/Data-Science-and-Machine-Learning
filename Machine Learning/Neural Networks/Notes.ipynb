{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Neural Networks.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"GwJiHMiqEUV8"},"source":["# **Neural Networks**"]},{"cell_type":"markdown","metadata":{"id":"6KVhBmWF22_w"},"source":["**What will you learn?**\r\n","1. **Introduction :** Intro to ANNs\r\n","2. **Why do we need NN?**\r\n","3. **Example with Linear Boundaries** : Negation, AND, OR\r\n","3. **Example with Non-Linear Boundaries** : XOR\r\n","5. **Terminology**\r\n","6. **Propogation**\r\n","7. **Cost Function**\r\n","8. **Multiclass Classification** : One Hot Encoding\r\n","9. **Sklearn Implementation** : MLPClassifier"]},{"cell_type":"markdown","metadata":{"id":"klI5B815EUS7"},"source":["##**Introdution**\r\n"]},{"cell_type":"markdown","metadata":{"id":"rcYZlGtYEUQa"},"source":["Artificial Neural Networks are one of the main tools used in machine learning. As the “neural” part of their name suggests, they are brain-inspired systems which are intended to replicate the way that we humans learn. Neural networks consist of input and output layers, as well as (in most cases) a hidden layer consisting of units that transform the input into something that the output layer can use. They are excellent tools for finding patterns which are far too complex or numerous for a human programmer to extract and teach the machine to recognize.\r\n","\r\n","For a basic idea of how a deep learning neural network learns, imagine a factory line. After the raw materials (the data set) are input, they are then passed down the conveyer belt, with each subsequent stop or layer extracting a different set of high-level features. If the network is intended to recognize an object, the first layer might analyze the brightness of its pixels.\r\n","\r\n","The next layer could then identify any edges in the image, based on lines of similar pixels. After this, another layer may recognize textures and shapes, and so on. By the time the fourth or fifth layer is reached, the deep learning net will have created complex feature detectors. It can figure out that certain image elements (such as a pair of eyes, a nose, and a mouth) are commonly found together.\r\n","\r\n","Once this is done, the researchers who have trained the network can give labels to the output, and then use backpropagation to correct any mistakes which have been made. After a while, the network can carry out its own classification tasks without needing humans to help every time.\r\n","\r\n","<img src = \"https://files.codingninjas.in/3nn-7659.gif\" width = 800>\r\n","\r\n"]},{"cell_type":"markdown","metadata":{"id":"sUitQBD_EUOH"},"source":["##**Why do we need NN?**"]},{"cell_type":"markdown","metadata":{"id":"5GVwtToUEUL2"},"source":["Neural Networks have been around even before machine learning gained pace. But they were thought to be computationally too heavy and hence, brushed aside.\r\n","\r\n","A problem we faced during Logistic Regression was that, even though the decision function (Sigmoid) was non linear, we got a linear decision boundary. We fixed this problem by adding dummy data with higher powers.\r\n","\r\n","To do that, we had to experiment and decide the degree of features we needed to add. Our decision boundary shoud be such, that it performs this task on its own."]},{"cell_type":"markdown","metadata":{"id":"3cs3bFPdEUJj"},"source":["Logistic regression had the following structure:\r\n","\r\n","<img src = \"https://files.codingninjas.in/nn1-7661.jpg\" width = 500>\r\n","\r\n","The intuition behind Neural Networks is a follows:\r\n","\r\n","<img src = \"\thttps://files.codingninjas.in/nn2-7662.jpg\" width = 500>\r\n","\r\n","So, here the final output will not be linear with respect to $x_1, x_2, x_0$.\r\n","The functions $f_1, f_2$ need not necessarily be Sigmoid. We can choose any function. Using this method we can create quite interesting decision boundaries without applying the dummy feature method."]},{"cell_type":"markdown","metadata":{"id":"nYhfUp19EUHJ"},"source":["##**Example with Linear Decision Boundaries**"]},{"cell_type":"markdown","metadata":{"id":"tufpDdojEUEt"},"source":["To understand how to reach the boundaries, lets take a simple example"]},{"cell_type":"markdown","metadata":{"id":"oYPUnaBlEUCO"},"source":["###**Example 1 : Negation**"]},{"cell_type":"markdown","metadata":{"id":"Aal8ton6ET_q"},"source":["x | y | \r\n",":---:|:---:|\r\n","1|0|\r\n","0|1|"]},{"cell_type":"markdown","metadata":{"id":"ekTFRxne3p1D"},"source":["<img src = \"https://files.codingninjas.in/negation-7688.jpg\" width = 450>"]},{"cell_type":"markdown","metadata":{"id":"p3WCQyWIET9S"},"source":["Here, the function used is \r\n","$$\\frac{1}{1 + e^{-z}}$$\r\n","\r\n","We want to pick the correct values of $w_0$ and $w_1$ so that we reach to the correct answer.\r\n","\r\n","**Case 1** : When $x = 0$, we want $y = 1$. So, we want $z\\geq0$.\r\n","$$w_0 + w_1x \\geq 0$$\r\n","$$w_0 \\geq 0$$\r\n","\r\n","This means we need to keep $w_0$ at a high value, so sigmoid function closely reaches 1. Lets take $w_0 = 50$.\r\n","\r\n","Hence, $z = 50$, which is what we wanted.\r\n","\r\n","**Case 2** : When $x = 1$, we want $y = 0$. So, we want $z\\geq0$.\r\n","$$w_0 + w_1x \\leq 0$$\r\n","$$w_0 + w_1 \\leq 0$$\r\n","\r\n","Lets take $w_1$ = -100\r\n","\r\n","Hence, $z = -50$, which is what we wanted."]},{"cell_type":"markdown","metadata":{"id":"Q0SF7mgmET66"},"source":["###**Example 2 : OR**"]},{"cell_type":"markdown","metadata":{"id":"lQS2t_LT4I8S"},"source":["<img src = \"https://files.codingninjas.in/or-7690.jpg\" width = 500>"]},{"cell_type":"markdown","metadata":{"id":"uKbiStsbET4l"},"source":["$x_1$ | $x_2$ | $y$ \r\n",":---:|:---:|:---:|\r\n","0|0|0\r\n","1|0|1\r\n","0|1|1\r\n","1|1|1"]},{"cell_type":"markdown","metadata":{"id":"9iJSHtchET2C"},"source":["Here, the function used is \r\n","$$\\frac{1}{1 + e^{-z}}$$\r\n","\r\n","We want to pick the correct values of $w_0$ and $w_1$ so that we reach to the correct answer.\r\n","\r\n","**Case 1** : When $x_1 = 0$ and $x_2 = 0$, we want $y = 0$.\r\n","\r\n","Therefore,\r\n","$$ w_1x_1 + w_2x_2 + w_0(1) < 0 $$\r\n","$$ w_1(0) + w_2(0) + w_0 < 0 $$\r\n","$$ w_0 < 0 $$\r\n","\r\n","Lets take $w_0 = -50$\r\n","\r\n","**Case 2** : When $x_1 = 1$ and $x_2 = 0$, we want $y = 1$.\r\n","\r\n","Therefore,\r\n","$$ w_1x_1 + w_2x_2 + w_0(1) > 0 $$\r\n","$$ w_1(1) + w_2(0) + w_0 > 0 $$\r\n","$$ w_0 + w_1 > 0 $$\r\n","\r\n","Lets take $w_1 = 100$\r\n","\r\n","**Case 3** : When $x_1 = 0$ and $x_2 = 1$, we want $y = 1$.\r\n","\r\n","Therefore,\r\n","$$ w_1x_1 + w_2x_2 + w_0(1) > 0 $$\r\n","$$ w_1(0) + w_2(1) + w_0 > 0 $$\r\n","$$ w_0 + w_2 > 0 $$\r\n","\r\n","Lets take $w_2 = 100$\r\n","\r\n","So, we can draw the table as :\r\n","\r\n","$x_1$ | $x_2$ | $y$ | $z$ | $y_p$\r\n",":---:|:---:|:---:|:---:|:---:|\r\n","0|0|0|-50|0\r\n","1|0|1|50|1\r\n","0|1|1|50|1\r\n","1|1|1|150|1"]},{"cell_type":"markdown","metadata":{"id":"yj9ZK9E6ETzr"},"source":["Try doing the calculations for **AND** Gate\r\n","\r\n","$x_1$ | $x_2$ | $y$ \r\n",":---:|:---:|:---:|\r\n","0|0|0\r\n","1|0|0\r\n","0|1|0\r\n","1|1|1"]},{"cell_type":"markdown","metadata":{"id":"EYPsAqM4ETxU"},"source":["##**Example with Non Linear Decision Boundaries**"]},{"cell_type":"markdown","metadata":{"id":"u2ghrW2AETvB"},"source":["###**Example 1 : XOR**\r\n","\r\n","$x_1$ | $x_2$ | XOR | AND | NOR\r\n",":---:|:---:|:---:|:---:|:---:|\r\n","0|0|0|0|1\r\n","1|0|1|0|0\r\n","0|1|1|0|0\r\n","1|1|0|1|0\r\n","\r\n","If we look at the table closely, when outputs of AND and NOR are 0, XOR is 1.\r\n","If any of AND and NOR is 1, output of XOR is 0.\r\n","\r\n","So we can combine AND and NOR to reach XOR.\r\n","Taking AND to be $f_1$ and NOR to be $f_2$ we can say that NOR($f_1$, $f_2$) will give the desired output.\r\n","\r\n","<img src = \"\thttps://files.codingninjas.in/xor-7691.jpg\">\r\n","\r\n","Verify the results with your own calculations."]},{"cell_type":"markdown","metadata":{"id":"1GxhGeJKuQds"},"source":["##**Terminology**"]},{"cell_type":"markdown","metadata":{"id":"1rrtbsPKuQbR"},"source":["**Neuron** : A single unit in any layer is called neuron.\r\n","\r\n","**Input Layer** : The Input layer communicates with the external environment that presents a pattern to the neural network. Its job is to deal with all the inputs only.The input layer should represent the condition for which we are training the neural network. Every input neuron should represent some independent variable that has an influence over the output of the neural network.\r\n","\r\n","**Hidden Layer** : The hidden layer is the collection of neurons which has activation function applied on it and it is an intermediate layer found between the input layer and the output layer. Its job is to process the inputs obtained by its previous layer. So it is the layer which is responsible extracting the required features from the input data.\r\n","\r\n","\r\n","**Output Layer** : The output layer of the neural network collects and transmits the information accordingly in way it has been designed to give. The pattern presented by the output layer can be directly traced back to the input layer. The number of neurons in output layer should be directly related to the type of work that the neural network was performing.\r\n","\r\n","\r\n","Weights for each neuron will be found using some algorithm. What we need to decide is :\r\n","1. How many hidden layers we want?\r\n","2. How many neurons in each layer?\r\n","3. Function to be applied over hidden and output layer.\r\n","\r\n","\r\n","<img src = \"\thttps://files.codingninjas.in/network-7689.png\">"]},{"cell_type":"markdown","metadata":{"id":"DAUccCwNuQYs"},"source":["##**Propogation**\r\n"]},{"cell_type":"markdown","metadata":{"id":"EgMv4m2MuQWC"},"source":["###**Forward Propagation**"]},{"cell_type":"markdown","metadata":{"id":"UZsOXPtAuQTE"},"source":["The input X provides the initial information that then propagates to the hidden units at each layer and finally produce the output y^. The architecture of the network entails determining its depth, width, and activation functions used on each layer. Depth is the number of hidden layers. Width is the number of units (nodes) on each hidden layer since we don’t control neither input layer nor output layer dimensions. There are quite a few set of activation functions such Rectified Linear Unit, Sigmoid, Hyperbolic tangent, etc. Research has proven that deeper networks outperform networks with more hidden units. Therefore, it’s always better and won’t hurt to train a deeper network (with diminishing returns)."]},{"cell_type":"markdown","metadata":{"id":"fI1U2vy6uQQV"},"source":["###**Backward Propagation**"]},{"cell_type":"markdown","metadata":{"id":"twtjzRoAuQM3"},"source":["Backpropagation refers to the method of calculating the gradient of neural network parameters. In short, the method traverses the network in reverse order, from the output to the input layer, according to the chain rule from calculus. The algorithm stores any intermediate variables (partial derivatives) required while calculating the gradient with respect to some parameters."]},{"cell_type":"markdown","metadata":{"id":"kcauU6eZuQKK"},"source":["##**Cost Function**"]},{"cell_type":"markdown","metadata":{"id":"zjnSOq5zuQHh"},"source":["For neural networks, or any other algorithm for that matter, the cost function is similar. Here, \r\n","\r\n","$$ Cost = Error + \\lambda Regularisation$$\r\n","\r\n","For regularisation, we will use $\\sum w_j^2$, which is sum of all the weights squared.\r\n","\r\n","Now, error function can be :\r\n","\r\n","$$ (y_t - y_{pred})^2 $$\r\n","\r\n","Therefore, \r\n","\r\n","$$ Cost = \\frac{1}{m}\\sum(y_t - y_{pred})^2 + \\frac{\\lambda}{2m} \\sum w_j^2$$"]},{"cell_type":"markdown","metadata":{"id":"RedvC5J3uQE1"},"source":["##**How to handle Multiclass Classification**\r\n","\r\n"]},{"cell_type":"markdown","metadata":{"id":"iALK3tv7uQB9"},"source":["To handle this, basic idea is to add additional weights. All remains same, but at output layer also, multiple units are added.\r\n","\r\n","Lets assume that there are 3 classes that can be predicted.\r\n","\r\n","<img src = \"https://files.codingninjas.in/multiclass-7687.png\">\r\n","\r\n","Lets say the values predicted are:\r\n","$y_1 = 0.1$, $y_2 = 0.15$ and  $y_3 = 0.99$.\r\n","\r\n","We will say that the data points belong to the max value class, in this case Class 3. The true value of the output will be in the form of a vector like [0, 0, 1].\r\n","\r\n","Now, for above model, data points will also be in form of a vector, as is output.\r\n","\r\n","If datapoint $x^1$ belongs to the 1st class, then its input vector is [1, 0, 0].\r\n","\r\n","\r\n","Similarily, if $x^2$ belongs to the 3rd class, then its input vector is [0, 0, 1].\r\n","\r\n","Such an input is called **One Hot Encoded** input.\r\n","\r\n","\r\n","Cost function changes to :\r\n","\r\n","$$Cost = \\sum^m_{i = 1} \\sum^k_{j = 1} f(y_{i}^j(pred),\\enspace y_i^j(true)) + \\frac{\\lambda}{2m} \\sum w_j^2 $$\r\n","\r\n","This extra summation $\\sum^k_{j = 1}$ penalises us if one hot encoding is incorrect. Hence, error and cost are not just to be calculated for correct units, but also for incorrect prediction of other units."]},{"cell_type":"markdown","metadata":{"id":"fHZL753f0oWZ"},"source":["##**MLP Classifier in Sklearn**"]},{"cell_type":"markdown","metadata":{"id":"3y0-4fxE0oTn"},"source":["The MLP classifier is not a very efficient classifier. It is not advised to use in implementaion of neural networks of large data or an actual product."]},{"cell_type":"code","metadata":{"id":"s7fB4M_x1VnT"},"source":["from sklearn import datasets\r\n","from sklearn.model_selection import train_test_split\r\n","from sklearn.neural_network import MLPClassifier"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gNQtJuFa1Vk5","executionInfo":{"status":"ok","timestamp":1611600021301,"user_tz":-330,"elapsed":5136,"user":{"displayName":"Gaurav Bhatia","photoUrl":"","userId":"05517600112429710610"}},"outputId":"caea82e7-f5be-446b-ef22-e710648a5aff"},"source":["clf  =  MLPClassifier()  # Creating object \r\n","iris = datasets.load_iris()  # Loading dataset\r\n","X = iris.data\r\n","Y = iris.target\r\n","xtrain, xtest, ytrain, ytest = train_test_split(X, Y)\r\n","clf.fit(xtrain, ytrain)  # Training neural network "],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n","  % self.max_iter, ConvergenceWarning)\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n","              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n","              hidden_layer_sizes=(100,), learning_rate='constant',\n","              learning_rate_init=0.001, max_fun=15000, max_iter=200,\n","              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n","              power_t=0.5, random_state=None, shuffle=True, solver='adam',\n","              tol=0.0001, validation_fraction=0.1, verbose=False,\n","              warm_start=False)"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hUMGOWl_1VBL","executionInfo":{"status":"ok","timestamp":1611600021302,"user_tz":-330,"elapsed":5007,"user":{"displayName":"Gaurav Bhatia","photoUrl":"","userId":"05517600112429710610"}},"outputId":"57ca6d35-8dd4-44f9-e5fe-ea1f81ea679b"},"source":["clf.score(xtest,ytest) # Obtaining score "],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.9210526315789473"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uh_ESEQ61U-5","executionInfo":{"status":"ok","timestamp":1611600021303,"user_tz":-330,"elapsed":4548,"user":{"displayName":"Gaurav Bhatia","photoUrl":"","userId":"05517600112429710610"}},"outputId":"fe9dcf2d-e3b2-4731-a889-65b8e346c2da"},"source":["clf.predict(xtest)   # results"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([1, 2, 1, 2, 0, 1, 2, 0, 2, 2, 2, 1, 1, 2, 2, 2, 0, 2, 2, 1, 0, 0,\n","       0, 1, 1, 2, 0, 2, 2, 1, 0, 0, 2, 2, 2, 0, 1, 2])"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"markdown","metadata":{"id":"apN3R8d40oQs"},"source":["###**Important Parameters**\r\n","**hidden_layer_sizes : tuple, length = n_layers - 2, default=(100,)**\r\n","The ith element represents the number of neurons in the ith hidden layer.\r\n","\r\n","**activation : {‘identity’, ‘logistic’, ‘tanh’, ‘relu’}, default=’relu’**\r\n","Activation function for the hidden layer.\r\n","\r\n","‘identity’, no-op activation, useful to implement linear bottleneck, returns f(x) = x\r\n","\r\n","‘logistic’, the logistic sigmoid function, returns f(x) = 1 / (1 + exp(-x)).\r\n","\r\n","‘tanh’, the hyperbolic tan function, returns f(x) = tanh(x).\r\n","\r\n","‘relu’, the rectified linear unit function, returns f(x) = max(0, x)\r\n","\r\n","**batch_size : int, default=’auto’**\r\n","Size of minibatches for stochastic optimizers. If the solver is ‘lbfgs’, the classifier will not use minibatch. When set to “auto”, batch_size=min(200, n_samples)"]},{"cell_type":"markdown","metadata":{"id":"rq7e8Pr-0oN_"},"source":["###**Important Attributes**\r\n","**coefs_ : list of shape (n_layers - 1,)**\r\n","The ith element in the list represents the weight matrix corresponding to layer i.\r\n","\r\n","**intercepts_ : list of shape (n_layers - 1,)**\r\n","The ith element in the list represents the bias vector corresponding to layer i + 1."]}]}