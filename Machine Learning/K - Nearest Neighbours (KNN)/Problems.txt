
1. K = 1 leads to :
Answer: Overfitting


2. It is advisable to take odd values of k while using KNN classifier to:
Answer: avoid ties


3. Suppose, you have given the following data where x and y are the 2 input variables and Class is the dependent variable. Below is a scatter plot which shows the above data in 2D space. Suppose, you want to predict the class of new data point x=1 and y=1 using Euclidean distance in 3-NN. In which class this data point belongs to?
Answer: 3 nearest points to (1,1)  are [(1,0) representing(+)] ,[(1,2) representing(+)],[(0,1)representing(+)]. Therefore prediction of data point (1,1) is (+) class


4. Suppose, you have given the following data where x and y are the 2 input variables and Class is the dependent variable. Below is a scatter plot which shows the above data in 2D space. Suppose, you want to predict the class of new data point x=1 and y=1 using Euclidean distance in 7-NN. In which class this data point belongs to?
Answer: 7 nearest neighbours include all points except 2,3 which is at a euclidean distance of sqrt(5) . Therefore majority are (-) , hence in case of 7NN , prediction of  (1,1) is  class (-)


5. KNN does more computation on test time than on train time?
Answer: There is no explicit training phase in KNN


6. What is the default value of k in KNN implemented in sklearn?
Answer: 5


7. For k cross-validation, a smaller k value implies ____
Answer: less variance


8. Which of the following is the correct use of cross-validation?
Answer: Selecting variables to include in a model, Comparing predictors, Selecting parameters in prediction function


9. Which of the following is the default cross-validation splitting strategy used in the cross_val_score of sklearn?
Answer: 3-fold


10. “Accuracy_score” is imported from which of the following:
Answer: Sklearn.metrics


11. For two points (a,b) and (x,y) in 2D space, (|a-x|+|b-y|) represents which distance?
Answer: manhattan distance


12. Which of the following techniques is used generally for feature selection in case of KNN?
Answer: Back elimination algorithm


13. If we decide to give weights to manage our features in KNN and we have our data as shown below, what might possibly be the weights assigned to feature1 and feature2? There are other features also present in the data set which are not shown for clarity purposes. Assume that weights vary between 0 and 100. Max weight is 100 and min weight is 0.
Answer: (95 , 5), (5 , 95)


14. In which of the following case(s), do we need to handle separately to use KNN?
Answer: Categorical data which has a feature, which can have values that don’t have natural ordering


15. Which of the following is the best way to handle categorical data which has a feature that can have any value from {cricket, hockey, basketball} ?
Answer: make separate columns for cricket,basketball and hockey with binary entry representing if the sport occurs for the data point or not


16. A KD Tree(also called a K-Dimensional Tree) is a binary search tree where data in each node is a K-Dimensional point in space. In short, it is a space partitioning data structure for organising points in a K-Dimensional space. It is used in the Nearest Neighbours algorithm. True or False?
Answer: True


17. We are doing O(n) work on each data point in our implemented brute force method. Till which of the following will KD-Tree reduce the work done on each data-point?
Answer: O(log(n))


18. Select the options which are among the pros of KNNs
Answer: Versatile - useful for Classification and Regression, Simple algorithm


19. Select the options which are among the cons of KNNs
Answer: Computationally expensive, High memory requirement, Predicting stage might be slow
