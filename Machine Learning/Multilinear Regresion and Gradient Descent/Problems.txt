
1. You run gradient descent for 20 iterations with α=0.3 and compute cost function after each iteration. You find that the value of cost function decreases slowly and is still decreasing after 20 iterations. Based on this, which of the following conclusions seems most plausible ?
Answer: It'd be more wise to try a larger value of α (say α=1.0)


2. If learning rate is high:
Answer: There is a of risk Overshooting the lowest point


3. With a very low learning rate: 
Answer: A low learning rate is more precise, Calculating the gradient is time-consuming, so it will take us a very long time to get to the bottom


4. We compute the cost gradient based on the complete training set; hence, we sometimes also call it________. In case of very large datasets, using this gradient descent can be quite costly since we are only taking a single step for one pass over the training set.
Answer: Batch gradient descent


5. Do proper match for below statements:
	1. Uses n data points instead of 1 sample at each iteration.
	2. Computes the gradient using a single sample.
	3. Computes the gradient using the whole dataset.
Types of Gradient Descent
	A: Mini-batch gradient descent 
	B: Stochastic gradient descent 
	C: Batch gradient descent
Answer: 3 --- > C, 2 --- > B, 1 ---> A
 