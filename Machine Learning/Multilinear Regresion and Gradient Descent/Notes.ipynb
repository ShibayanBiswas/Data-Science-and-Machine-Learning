{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "681a6a8f",
   "metadata": {},
   "source": [
    "## **Limitations of Linear Regression**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f3a5e8",
   "metadata": {},
   "source": [
    "Linear regression, though a very powerful algorithm, has certain disadvantages\n",
    "1. **Main limitation of Linear Regression** is the assumption of linearity between the dependent variable and the independent variables. In the real world, the data is almost never linearly separable. The assumption that there is a straight line relationship is usually wrong.\n",
    "\n",
    "2. **Prone to noise and overfitting:** If the number of observations are lesser than the number of features, Linear Regression should not be used, otherwise it may lead to overfit, and the relationship thus formed will be noisy.\n",
    "\n",
    "3. **Prone to outliers:** Linear regression is very sensitive to outliers. An outlier can be considered as an anomaly. It refers to a datapoint which has no clear relationship with any other data point in the data. So, outliers should be analyzed and removed before applying Linear Regression to the dataset, or the linear relationship formed would be highly skewed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6488beb6",
   "metadata": {},
   "source": [
    "Let's explore the first limitation in detail. Have a look at the following graph:\n",
    "\n",
    "<img src=\"https://files.codingninjas.in/graph_gd-7051.jpg\" width=\"350\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7a1c71",
   "metadata": {},
   "source": [
    "In the above figure, it is quite clear, that the linear line formed will not correctly predict the results of data points(shown in blue).\n",
    "\n",
    "Thus, we need to plot more complex boundaries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d020aa01",
   "metadata": {},
   "source": [
    "## **Plotting More Complex Boundaries**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411c4d40",
   "metadata": {},
   "source": [
    "As we have learnt in the previous modules, **Function or Hypothesis** of Linear Regression is represented by -\n",
    "$y = m_1.x_1 + m_2.x_2 + m_3.x_3 + … + m_n.x_n + b$.\n",
    "This is essentially a line of the form $ y = mx + c$.\n",
    "\n",
    "To plot more complex boundaries, we need to have an equation of higher degree. For example:\n",
    "1. $ y = m_1x^2 + m_2x + c $\n",
    "2. $ y = m_1x^3 + m_2x^2 + m_3x + c$\n",
    "\n",
    "So, the basic idea to do this, is to add dummy features in our dataset. Suppose the current data set has three features: $x_1, x_2 $ and $x_3$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b079bfb0",
   "metadata": {},
   "source": [
    "The equation for the above dataset would be :\n",
    "\n",
    "$y = m_1.x_1 + m_2.x_2 + m_3.x_3 + b$.\n",
    "\n",
    "We can add dummy features to our dataset by enhancing the already existing features. One of the most common method is to create a feature, which is the product of already existing features. Let us create a new feature, $x_{12}$ which is the product of $x_1$ and $x_2$. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6252a289",
   "metadata": {},
   "source": [
    "The equation for the above dataset would be :\n",
    "\n",
    "$y = m_1.x_1 + m_2.x_2 + m_3.x_3 + m_4.x_{12}+ b$.\n",
    "\n",
    "where the term $x_{12}$ is essentially a quadratic term.\n",
    "\n",
    "We can add more features like $x_{31}$ and $x_{23}$. And we are not just limited to quadratic terms, we may also add cubic terms like $x_{123}$.\n",
    "Other logarithimic and trignometric functions may also be used to plot more complex boundaries.\n",
    "\n",
    "Let's look at an example of how to add a feature into our dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ebe904",
   "metadata": {},
   "source": [
    "### **Example on how to code complex boundaries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef4a6654",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee73b5a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   8.58,   38.38, 1021.03,   84.37],\n",
       "       [  21.79,   58.2 , 1017.21,   66.74],\n",
       "       [  16.64,   48.92, 1011.55,   78.76],\n",
       "       ...,\n",
       "       [  29.8 ,   69.34, 1009.36,   64.74],\n",
       "       [  16.37,   54.3 , 1017.94,   63.63],\n",
       "       [  30.11,   62.04, 1010.69,   47.96]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = np.loadtxt(\"https://files.codingninjas.in/0000000000002419_training_ccpp_x_y_train-7050.csv\", delimiter=\",\")\n",
    "x = train_data[:,:-1]\n",
    "y = train_data[:,-1]\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4575cee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.58</td>\n",
       "      <td>38.38</td>\n",
       "      <td>1021.03</td>\n",
       "      <td>84.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21.79</td>\n",
       "      <td>58.20</td>\n",
       "      <td>1017.21</td>\n",
       "      <td>66.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16.64</td>\n",
       "      <td>48.92</td>\n",
       "      <td>1011.55</td>\n",
       "      <td>78.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31.38</td>\n",
       "      <td>71.32</td>\n",
       "      <td>1009.17</td>\n",
       "      <td>60.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9.20</td>\n",
       "      <td>40.03</td>\n",
       "      <td>1017.05</td>\n",
       "      <td>92.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7171</th>\n",
       "      <td>9.32</td>\n",
       "      <td>37.73</td>\n",
       "      <td>1022.14</td>\n",
       "      <td>79.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7172</th>\n",
       "      <td>11.20</td>\n",
       "      <td>41.38</td>\n",
       "      <td>1021.65</td>\n",
       "      <td>61.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7173</th>\n",
       "      <td>29.80</td>\n",
       "      <td>69.34</td>\n",
       "      <td>1009.36</td>\n",
       "      <td>64.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7174</th>\n",
       "      <td>16.37</td>\n",
       "      <td>54.30</td>\n",
       "      <td>1017.94</td>\n",
       "      <td>63.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7175</th>\n",
       "      <td>30.11</td>\n",
       "      <td>62.04</td>\n",
       "      <td>1010.69</td>\n",
       "      <td>47.96</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7176 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         x1     x2       x3     x4\n",
       "0      8.58  38.38  1021.03  84.37\n",
       "1     21.79  58.20  1017.21  66.74\n",
       "2     16.64  48.92  1011.55  78.76\n",
       "3     31.38  71.32  1009.17  60.42\n",
       "4      9.20  40.03  1017.05  92.46\n",
       "...     ...    ...      ...    ...\n",
       "7171   9.32  37.73  1022.14  79.49\n",
       "7172  11.20  41.38  1021.65  61.89\n",
       "7173  29.80  69.34  1009.36  64.74\n",
       "7174  16.37  54.30  1017.94  63.63\n",
       "7175  30.11  62.04  1010.69  47.96\n",
       "\n",
       "[7176 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "li = ['x1','x2','x3','x4']\n",
    "df = pd.DataFrame(x)\n",
    "df.columns = li\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9eaddf6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x1_x1</th>\n",
       "      <th>x1_x2</th>\n",
       "      <th>x1_x3</th>\n",
       "      <th>x1_x4</th>\n",
       "      <th>x2_x2</th>\n",
       "      <th>x2_x3</th>\n",
       "      <th>x2_x4</th>\n",
       "      <th>x3_x3</th>\n",
       "      <th>x3_x4</th>\n",
       "      <th>x4_x4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.58</td>\n",
       "      <td>38.38</td>\n",
       "      <td>1021.03</td>\n",
       "      <td>84.37</td>\n",
       "      <td>73.6164</td>\n",
       "      <td>329.3004</td>\n",
       "      <td>8760.4374</td>\n",
       "      <td>723.8946</td>\n",
       "      <td>1473.0244</td>\n",
       "      <td>39187.1314</td>\n",
       "      <td>3238.1206</td>\n",
       "      <td>1.042502e+06</td>\n",
       "      <td>86144.3011</td>\n",
       "      <td>7118.2969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21.79</td>\n",
       "      <td>58.20</td>\n",
       "      <td>1017.21</td>\n",
       "      <td>66.74</td>\n",
       "      <td>474.8041</td>\n",
       "      <td>1268.1780</td>\n",
       "      <td>22165.0059</td>\n",
       "      <td>1454.2646</td>\n",
       "      <td>3387.2400</td>\n",
       "      <td>59201.6220</td>\n",
       "      <td>3884.2680</td>\n",
       "      <td>1.034716e+06</td>\n",
       "      <td>67888.5954</td>\n",
       "      <td>4454.2276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16.64</td>\n",
       "      <td>48.92</td>\n",
       "      <td>1011.55</td>\n",
       "      <td>78.76</td>\n",
       "      <td>276.8896</td>\n",
       "      <td>814.0288</td>\n",
       "      <td>16832.1920</td>\n",
       "      <td>1310.5664</td>\n",
       "      <td>2393.1664</td>\n",
       "      <td>49485.0260</td>\n",
       "      <td>3852.9392</td>\n",
       "      <td>1.023233e+06</td>\n",
       "      <td>79669.6780</td>\n",
       "      <td>6203.1376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31.38</td>\n",
       "      <td>71.32</td>\n",
       "      <td>1009.17</td>\n",
       "      <td>60.42</td>\n",
       "      <td>984.7044</td>\n",
       "      <td>2238.0216</td>\n",
       "      <td>31667.7546</td>\n",
       "      <td>1895.9796</td>\n",
       "      <td>5086.5424</td>\n",
       "      <td>71974.0044</td>\n",
       "      <td>4309.1544</td>\n",
       "      <td>1.018424e+06</td>\n",
       "      <td>60974.0514</td>\n",
       "      <td>3650.5764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9.20</td>\n",
       "      <td>40.03</td>\n",
       "      <td>1017.05</td>\n",
       "      <td>92.46</td>\n",
       "      <td>84.6400</td>\n",
       "      <td>368.2760</td>\n",
       "      <td>9356.8600</td>\n",
       "      <td>850.6320</td>\n",
       "      <td>1602.4009</td>\n",
       "      <td>40712.5115</td>\n",
       "      <td>3701.1738</td>\n",
       "      <td>1.034391e+06</td>\n",
       "      <td>94036.4430</td>\n",
       "      <td>8548.8516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7171</th>\n",
       "      <td>9.32</td>\n",
       "      <td>37.73</td>\n",
       "      <td>1022.14</td>\n",
       "      <td>79.49</td>\n",
       "      <td>86.8624</td>\n",
       "      <td>351.6436</td>\n",
       "      <td>9526.3448</td>\n",
       "      <td>740.8468</td>\n",
       "      <td>1423.5529</td>\n",
       "      <td>38565.3422</td>\n",
       "      <td>2999.1577</td>\n",
       "      <td>1.044770e+06</td>\n",
       "      <td>81249.9086</td>\n",
       "      <td>6318.6601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7172</th>\n",
       "      <td>11.20</td>\n",
       "      <td>41.38</td>\n",
       "      <td>1021.65</td>\n",
       "      <td>61.89</td>\n",
       "      <td>125.4400</td>\n",
       "      <td>463.4560</td>\n",
       "      <td>11442.4800</td>\n",
       "      <td>693.1680</td>\n",
       "      <td>1712.3044</td>\n",
       "      <td>42275.8770</td>\n",
       "      <td>2561.0082</td>\n",
       "      <td>1.043769e+06</td>\n",
       "      <td>63229.9185</td>\n",
       "      <td>3830.3721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7173</th>\n",
       "      <td>29.80</td>\n",
       "      <td>69.34</td>\n",
       "      <td>1009.36</td>\n",
       "      <td>64.74</td>\n",
       "      <td>888.0400</td>\n",
       "      <td>2066.3320</td>\n",
       "      <td>30078.9280</td>\n",
       "      <td>1929.2520</td>\n",
       "      <td>4808.0356</td>\n",
       "      <td>69989.0224</td>\n",
       "      <td>4489.0716</td>\n",
       "      <td>1.018808e+06</td>\n",
       "      <td>65345.9664</td>\n",
       "      <td>4191.2676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7174</th>\n",
       "      <td>16.37</td>\n",
       "      <td>54.30</td>\n",
       "      <td>1017.94</td>\n",
       "      <td>63.63</td>\n",
       "      <td>267.9769</td>\n",
       "      <td>888.8910</td>\n",
       "      <td>16663.6778</td>\n",
       "      <td>1041.6231</td>\n",
       "      <td>2948.4900</td>\n",
       "      <td>55274.1420</td>\n",
       "      <td>3455.1090</td>\n",
       "      <td>1.036202e+06</td>\n",
       "      <td>64771.5222</td>\n",
       "      <td>4048.7769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7175</th>\n",
       "      <td>30.11</td>\n",
       "      <td>62.04</td>\n",
       "      <td>1010.69</td>\n",
       "      <td>47.96</td>\n",
       "      <td>906.6121</td>\n",
       "      <td>1868.0244</td>\n",
       "      <td>30431.8759</td>\n",
       "      <td>1444.0756</td>\n",
       "      <td>3848.9616</td>\n",
       "      <td>62703.2076</td>\n",
       "      <td>2975.4384</td>\n",
       "      <td>1.021494e+06</td>\n",
       "      <td>48472.6924</td>\n",
       "      <td>2300.1616</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7176 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         x1     x2       x3     x4     x1_x1      x1_x2       x1_x3  \\\n",
       "0      8.58  38.38  1021.03  84.37   73.6164   329.3004   8760.4374   \n",
       "1     21.79  58.20  1017.21  66.74  474.8041  1268.1780  22165.0059   \n",
       "2     16.64  48.92  1011.55  78.76  276.8896   814.0288  16832.1920   \n",
       "3     31.38  71.32  1009.17  60.42  984.7044  2238.0216  31667.7546   \n",
       "4      9.20  40.03  1017.05  92.46   84.6400   368.2760   9356.8600   \n",
       "...     ...    ...      ...    ...       ...        ...         ...   \n",
       "7171   9.32  37.73  1022.14  79.49   86.8624   351.6436   9526.3448   \n",
       "7172  11.20  41.38  1021.65  61.89  125.4400   463.4560  11442.4800   \n",
       "7173  29.80  69.34  1009.36  64.74  888.0400  2066.3320  30078.9280   \n",
       "7174  16.37  54.30  1017.94  63.63  267.9769   888.8910  16663.6778   \n",
       "7175  30.11  62.04  1010.69  47.96  906.6121  1868.0244  30431.8759   \n",
       "\n",
       "          x1_x4      x2_x2       x2_x3      x2_x4         x3_x3       x3_x4  \\\n",
       "0      723.8946  1473.0244  39187.1314  3238.1206  1.042502e+06  86144.3011   \n",
       "1     1454.2646  3387.2400  59201.6220  3884.2680  1.034716e+06  67888.5954   \n",
       "2     1310.5664  2393.1664  49485.0260  3852.9392  1.023233e+06  79669.6780   \n",
       "3     1895.9796  5086.5424  71974.0044  4309.1544  1.018424e+06  60974.0514   \n",
       "4      850.6320  1602.4009  40712.5115  3701.1738  1.034391e+06  94036.4430   \n",
       "...         ...        ...         ...        ...           ...         ...   \n",
       "7171   740.8468  1423.5529  38565.3422  2999.1577  1.044770e+06  81249.9086   \n",
       "7172   693.1680  1712.3044  42275.8770  2561.0082  1.043769e+06  63229.9185   \n",
       "7173  1929.2520  4808.0356  69989.0224  4489.0716  1.018808e+06  65345.9664   \n",
       "7174  1041.6231  2948.4900  55274.1420  3455.1090  1.036202e+06  64771.5222   \n",
       "7175  1444.0756  3848.9616  62703.2076  2975.4384  1.021494e+06  48472.6924   \n",
       "\n",
       "          x4_x4  \n",
       "0     7118.2969  \n",
       "1     4454.2276  \n",
       "2     6203.1376  \n",
       "3     3650.5764  \n",
       "4     8548.8516  \n",
       "...         ...  \n",
       "7171  6318.6601  \n",
       "7172  3830.3721  \n",
       "7173  4191.2676  \n",
       "7174  4048.7769  \n",
       "7175  2300.1616  \n",
       "\n",
       "[7176 rows x 14 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    for j in range(i, 4):\n",
    "        ele = li[i] + \"_\" + li[j]\n",
    "        df[ele] = df[li[i]]*df[li[j]]\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ffac39",
   "metadata": {},
   "source": [
    "In the above dataset, we had 4 existing features. Using them, we added 10 dummy features who have a quadratic degree. Similar methods can be used to add cubic degree too. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdff8f3f",
   "metadata": {},
   "source": [
    "**But remember! Don't try to add many extra dummy features, as it leads to overfitting the data, leading to incorrect results.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bcbb2617",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9fbc9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category = FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99c39f55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': array([[6.3200e-03, 1.8000e+01, 2.3100e+00, ..., 1.5300e+01, 3.9690e+02,\n",
       "         4.9800e+00],\n",
       "        [2.7310e-02, 0.0000e+00, 7.0700e+00, ..., 1.7800e+01, 3.9690e+02,\n",
       "         9.1400e+00],\n",
       "        [2.7290e-02, 0.0000e+00, 7.0700e+00, ..., 1.7800e+01, 3.9283e+02,\n",
       "         4.0300e+00],\n",
       "        ...,\n",
       "        [6.0760e-02, 0.0000e+00, 1.1930e+01, ..., 2.1000e+01, 3.9690e+02,\n",
       "         5.6400e+00],\n",
       "        [1.0959e-01, 0.0000e+00, 1.1930e+01, ..., 2.1000e+01, 3.9345e+02,\n",
       "         6.4800e+00],\n",
       "        [4.7410e-02, 0.0000e+00, 1.1930e+01, ..., 2.1000e+01, 3.9690e+02,\n",
       "         7.8800e+00]]),\n",
       " 'target': array([24. , 21.6, 34.7, 33.4, 36.2, 28.7, 22.9, 27.1, 16.5, 18.9, 15. ,\n",
       "        18.9, 21.7, 20.4, 18.2, 19.9, 23.1, 17.5, 20.2, 18.2, 13.6, 19.6,\n",
       "        15.2, 14.5, 15.6, 13.9, 16.6, 14.8, 18.4, 21. , 12.7, 14.5, 13.2,\n",
       "        13.1, 13.5, 18.9, 20. , 21. , 24.7, 30.8, 34.9, 26.6, 25.3, 24.7,\n",
       "        21.2, 19.3, 20. , 16.6, 14.4, 19.4, 19.7, 20.5, 25. , 23.4, 18.9,\n",
       "        35.4, 24.7, 31.6, 23.3, 19.6, 18.7, 16. , 22.2, 25. , 33. , 23.5,\n",
       "        19.4, 22. , 17.4, 20.9, 24.2, 21.7, 22.8, 23.4, 24.1, 21.4, 20. ,\n",
       "        20.8, 21.2, 20.3, 28. , 23.9, 24.8, 22.9, 23.9, 26.6, 22.5, 22.2,\n",
       "        23.6, 28.7, 22.6, 22. , 22.9, 25. , 20.6, 28.4, 21.4, 38.7, 43.8,\n",
       "        33.2, 27.5, 26.5, 18.6, 19.3, 20.1, 19.5, 19.5, 20.4, 19.8, 19.4,\n",
       "        21.7, 22.8, 18.8, 18.7, 18.5, 18.3, 21.2, 19.2, 20.4, 19.3, 22. ,\n",
       "        20.3, 20.5, 17.3, 18.8, 21.4, 15.7, 16.2, 18. , 14.3, 19.2, 19.6,\n",
       "        23. , 18.4, 15.6, 18.1, 17.4, 17.1, 13.3, 17.8, 14. , 14.4, 13.4,\n",
       "        15.6, 11.8, 13.8, 15.6, 14.6, 17.8, 15.4, 21.5, 19.6, 15.3, 19.4,\n",
       "        17. , 15.6, 13.1, 41.3, 24.3, 23.3, 27. , 50. , 50. , 50. , 22.7,\n",
       "        25. , 50. , 23.8, 23.8, 22.3, 17.4, 19.1, 23.1, 23.6, 22.6, 29.4,\n",
       "        23.2, 24.6, 29.9, 37.2, 39.8, 36.2, 37.9, 32.5, 26.4, 29.6, 50. ,\n",
       "        32. , 29.8, 34.9, 37. , 30.5, 36.4, 31.1, 29.1, 50. , 33.3, 30.3,\n",
       "        34.6, 34.9, 32.9, 24.1, 42.3, 48.5, 50. , 22.6, 24.4, 22.5, 24.4,\n",
       "        20. , 21.7, 19.3, 22.4, 28.1, 23.7, 25. , 23.3, 28.7, 21.5, 23. ,\n",
       "        26.7, 21.7, 27.5, 30.1, 44.8, 50. , 37.6, 31.6, 46.7, 31.5, 24.3,\n",
       "        31.7, 41.7, 48.3, 29. , 24. , 25.1, 31.5, 23.7, 23.3, 22. , 20.1,\n",
       "        22.2, 23.7, 17.6, 18.5, 24.3, 20.5, 24.5, 26.2, 24.4, 24.8, 29.6,\n",
       "        42.8, 21.9, 20.9, 44. , 50. , 36. , 30.1, 33.8, 43.1, 48.8, 31. ,\n",
       "        36.5, 22.8, 30.7, 50. , 43.5, 20.7, 21.1, 25.2, 24.4, 35.2, 32.4,\n",
       "        32. , 33.2, 33.1, 29.1, 35.1, 45.4, 35.4, 46. , 50. , 32.2, 22. ,\n",
       "        20.1, 23.2, 22.3, 24.8, 28.5, 37.3, 27.9, 23.9, 21.7, 28.6, 27.1,\n",
       "        20.3, 22.5, 29. , 24.8, 22. , 26.4, 33.1, 36.1, 28.4, 33.4, 28.2,\n",
       "        22.8, 20.3, 16.1, 22.1, 19.4, 21.6, 23.8, 16.2, 17.8, 19.8, 23.1,\n",
       "        21. , 23.8, 23.1, 20.4, 18.5, 25. , 24.6, 23. , 22.2, 19.3, 22.6,\n",
       "        19.8, 17.1, 19.4, 22.2, 20.7, 21.1, 19.5, 18.5, 20.6, 19. , 18.7,\n",
       "        32.7, 16.5, 23.9, 31.2, 17.5, 17.2, 23.1, 24.5, 26.6, 22.9, 24.1,\n",
       "        18.6, 30.1, 18.2, 20.6, 17.8, 21.7, 22.7, 22.6, 25. , 19.9, 20.8,\n",
       "        16.8, 21.9, 27.5, 21.9, 23.1, 50. , 50. , 50. , 50. , 50. , 13.8,\n",
       "        13.8, 15. , 13.9, 13.3, 13.1, 10.2, 10.4, 10.9, 11.3, 12.3,  8.8,\n",
       "         7.2, 10.5,  7.4, 10.2, 11.5, 15.1, 23.2,  9.7, 13.8, 12.7, 13.1,\n",
       "        12.5,  8.5,  5. ,  6.3,  5.6,  7.2, 12.1,  8.3,  8.5,  5. , 11.9,\n",
       "        27.9, 17.2, 27.5, 15. , 17.2, 17.9, 16.3,  7. ,  7.2,  7.5, 10.4,\n",
       "         8.8,  8.4, 16.7, 14.2, 20.8, 13.4, 11.7,  8.3, 10.2, 10.9, 11. ,\n",
       "         9.5, 14.5, 14.1, 16.1, 14.3, 11.7, 13.4,  9.6,  8.7,  8.4, 12.8,\n",
       "        10.5, 17.1, 18.4, 15.4, 10.8, 11.8, 14.9, 12.6, 14.1, 13. , 13.4,\n",
       "        15.2, 16.1, 17.8, 14.9, 14.1, 12.7, 13.5, 14.9, 20. , 16.4, 17.7,\n",
       "        19.5, 20.2, 21.4, 19.9, 19. , 19.1, 19.1, 20.1, 19.9, 19.6, 23.2,\n",
       "        29.8, 13.8, 13.3, 16.7, 12. , 14.6, 21.4, 23. , 23.7, 25. , 21.8,\n",
       "        20.6, 21.2, 19.1, 20.6, 15.2,  7. ,  8.1, 13.6, 20.1, 21.8, 24.5,\n",
       "        23.1, 19.7, 18.3, 21.2, 17.5, 16.8, 22.4, 20.6, 23.9, 22. , 11.9]),\n",
       " 'feature_names': array(['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD',\n",
       "        'TAX', 'PTRATIO', 'B', 'LSTAT'], dtype='<U7'),\n",
       " 'DESCR': \".. _boston_dataset:\\n\\nBoston house prices dataset\\n---------------------------\\n\\n**Data Set Characteristics:**  \\n\\n    :Number of Instances: 506 \\n\\n    :Number of Attributes: 13 numeric/categorical predictive. Median Value (attribute 14) is usually the target.\\n\\n    :Attribute Information (in order):\\n        - CRIM     per capita crime rate by town\\n        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\\n        - INDUS    proportion of non-retail business acres per town\\n        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\\n        - NOX      nitric oxides concentration (parts per 10 million)\\n        - RM       average number of rooms per dwelling\\n        - AGE      proportion of owner-occupied units built prior to 1940\\n        - DIS      weighted distances to five Boston employment centres\\n        - RAD      index of accessibility to radial highways\\n        - TAX      full-value property-tax rate per $10,000\\n        - PTRATIO  pupil-teacher ratio by town\\n        - B        1000(Bk - 0.63)^2 where Bk is the proportion of black people by town\\n        - LSTAT    % lower status of the population\\n        - MEDV     Median value of owner-occupied homes in $1000's\\n\\n    :Missing Attribute Values: None\\n\\n    :Creator: Harrison, D. and Rubinfeld, D.L.\\n\\nThis is a copy of UCI ML housing dataset.\\nhttps://archive.ics.uci.edu/ml/machine-learning-databases/housing/\\n\\n\\nThis dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\\n\\nThe Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\\nprices and the demand for clean air', J. Environ. Economics & Management,\\nvol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\\n...', Wiley, 1980.   N.B. Various transformations are used in the table on\\npages 244-261 of the latter.\\n\\nThe Boston house-price data has been used in many machine learning papers that address regression\\nproblems.   \\n     \\n.. topic:: References\\n\\n   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\\n   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\\n\",\n",
       " 'filename': 'boston_house_prices.csv',\n",
       " 'data_module': 'sklearn.datasets.data'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading the dataset (get the features, output and description about the dataset)\n",
    "boston = datasets.load_boston() \n",
    "boston"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc19fa5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the input features (numpy array)\n",
    "X = boston.data\n",
    "\n",
    "# Getting the output (numpy array)\n",
    "Y = boston.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02129f21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(506, 13)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shape of the input features (it's a numpy array). Output : (rows, columns/features)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb5c9237",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.613524</td>\n",
       "      <td>11.363636</td>\n",
       "      <td>11.136779</td>\n",
       "      <td>0.069170</td>\n",
       "      <td>0.554695</td>\n",
       "      <td>6.284634</td>\n",
       "      <td>68.574901</td>\n",
       "      <td>3.795043</td>\n",
       "      <td>9.549407</td>\n",
       "      <td>408.237154</td>\n",
       "      <td>18.455534</td>\n",
       "      <td>356.674032</td>\n",
       "      <td>12.653063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>8.601545</td>\n",
       "      <td>23.322453</td>\n",
       "      <td>6.860353</td>\n",
       "      <td>0.253994</td>\n",
       "      <td>0.115878</td>\n",
       "      <td>0.702617</td>\n",
       "      <td>28.148861</td>\n",
       "      <td>2.105710</td>\n",
       "      <td>8.707259</td>\n",
       "      <td>168.537116</td>\n",
       "      <td>2.164946</td>\n",
       "      <td>91.294864</td>\n",
       "      <td>7.141062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.006320</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.460000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.385000</td>\n",
       "      <td>3.561000</td>\n",
       "      <td>2.900000</td>\n",
       "      <td>1.129600</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>187.000000</td>\n",
       "      <td>12.600000</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>1.730000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.082045</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.190000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.449000</td>\n",
       "      <td>5.885500</td>\n",
       "      <td>45.025000</td>\n",
       "      <td>2.100175</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>279.000000</td>\n",
       "      <td>17.400000</td>\n",
       "      <td>375.377500</td>\n",
       "      <td>6.950000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.256510</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.690000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.538000</td>\n",
       "      <td>6.208500</td>\n",
       "      <td>77.500000</td>\n",
       "      <td>3.207450</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>330.000000</td>\n",
       "      <td>19.050000</td>\n",
       "      <td>391.440000</td>\n",
       "      <td>11.360000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.677083</td>\n",
       "      <td>12.500000</td>\n",
       "      <td>18.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.624000</td>\n",
       "      <td>6.623500</td>\n",
       "      <td>94.075000</td>\n",
       "      <td>5.188425</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>666.000000</td>\n",
       "      <td>20.200000</td>\n",
       "      <td>396.225000</td>\n",
       "      <td>16.955000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>88.976200</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>27.740000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.871000</td>\n",
       "      <td>8.780000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>12.126500</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>711.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>396.900000</td>\n",
       "      <td>37.970000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             CRIM          ZN       INDUS        CHAS         NOX          RM  \\\n",
       "count  506.000000  506.000000  506.000000  506.000000  506.000000  506.000000   \n",
       "mean     3.613524   11.363636   11.136779    0.069170    0.554695    6.284634   \n",
       "std      8.601545   23.322453    6.860353    0.253994    0.115878    0.702617   \n",
       "min      0.006320    0.000000    0.460000    0.000000    0.385000    3.561000   \n",
       "25%      0.082045    0.000000    5.190000    0.000000    0.449000    5.885500   \n",
       "50%      0.256510    0.000000    9.690000    0.000000    0.538000    6.208500   \n",
       "75%      3.677083   12.500000   18.100000    0.000000    0.624000    6.623500   \n",
       "max     88.976200  100.000000   27.740000    1.000000    0.871000    8.780000   \n",
       "\n",
       "              AGE         DIS         RAD         TAX     PTRATIO           B  \\\n",
       "count  506.000000  506.000000  506.000000  506.000000  506.000000  506.000000   \n",
       "mean    68.574901    3.795043    9.549407  408.237154   18.455534  356.674032   \n",
       "std     28.148861    2.105710    8.707259  168.537116    2.164946   91.294864   \n",
       "min      2.900000    1.129600    1.000000  187.000000   12.600000    0.320000   \n",
       "25%     45.025000    2.100175    4.000000  279.000000   17.400000  375.377500   \n",
       "50%     77.500000    3.207450    5.000000  330.000000   19.050000  391.440000   \n",
       "75%     94.075000    5.188425   24.000000  666.000000   20.200000  396.225000   \n",
       "max    100.000000   12.126500   24.000000  711.000000   22.000000  396.900000   \n",
       "\n",
       "            LSTAT  \n",
       "count  506.000000  \n",
       "mean    12.653063  \n",
       "std      7.141062  \n",
       "min      1.730000  \n",
       "25%      6.950000  \n",
       "50%     11.360000  \n",
       "75%     16.955000  \n",
       "max     37.970000  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To move ahead with the data, we convert it into a dataframe\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(X)\n",
    "\n",
    "# By default, the dataframe is created in such a way where the row names are from (0 to 505) and the column names are from (0 to 12). We change the column names to the feature names of the dataset\n",
    "df.columns = boston.feature_names\n",
    "\n",
    "# describe the dataframe (to check if the features have string/NaN values)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d34cb7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into test and train data (randomly without any bias)\n",
    "from sklearn import model_selection\n",
    "\n",
    "# The random state parameter controls the shuffling process. Here we get the same train and test sets across different executions\n",
    "X_train, X_test, Y_train, Y_test = model_selection.train_test_split(X, Y, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "825545a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(379, 13)\n",
      "(127, 13)\n",
      "(379,)\n",
      "(127,)\n"
     ]
    }
   ],
   "source": [
    "# Checking the shape of the data (to check the percentage of data taken to train and test the algorithm)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_train.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "631f3c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using sklearn to import the classifier/algorithm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Creating the algorithm object that we can use to train and then test the data\n",
    "alg1 = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "783c6050",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting/training the data\n",
    "alg1.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d93b103d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Score : 0.7697699488741149\n",
      "Test Score : 0.6354638433202144\n"
     ]
    }
   ],
   "source": [
    "# Predicting the output\n",
    "Y_pred = alg1.predict(X_test)\n",
    "\n",
    "# Finding the Train Score\n",
    "train_score = alg1.score(X_train, Y_train)\n",
    "\n",
    "# Finding the Test Score\n",
    "test_score = alg1.score(X_test, Y_test)\n",
    "\n",
    "print(\"Train Score :\", train_score)\n",
    "print(\"Test Score :\", test_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6048419",
   "metadata": {},
   "source": [
    "For the Boston dataset, your task is to add all 2 degree features to the dataset and compare scores for the original dataset and new dataset with added features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "725c0d90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>...</th>\n",
       "      <th>TAX_TAX</th>\n",
       "      <th>TAX_PTRATIO</th>\n",
       "      <th>TAX_B</th>\n",
       "      <th>TAX_LSTAT</th>\n",
       "      <th>PTRATIO_PTRATIO</th>\n",
       "      <th>PTRATIO_B</th>\n",
       "      <th>PTRATIO_LSTAT</th>\n",
       "      <th>B_B</th>\n",
       "      <th>B_LSTAT</th>\n",
       "      <th>LSTAT_LSTAT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.00000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.613524</td>\n",
       "      <td>11.363636</td>\n",
       "      <td>11.136779</td>\n",
       "      <td>0.069170</td>\n",
       "      <td>0.554695</td>\n",
       "      <td>6.284634</td>\n",
       "      <td>68.574901</td>\n",
       "      <td>3.795043</td>\n",
       "      <td>9.549407</td>\n",
       "      <td>408.237154</td>\n",
       "      <td>...</td>\n",
       "      <td>195006.197628</td>\n",
       "      <td>7702.055336</td>\n",
       "      <td>138823.11502</td>\n",
       "      <td>5818.871146</td>\n",
       "      <td>345.284447</td>\n",
       "      <td>6547.619334</td>\n",
       "      <td>239.290334</td>\n",
       "      <td>135534.645253</td>\n",
       "      <td>4274.823238</td>\n",
       "      <td>210.993989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>8.601545</td>\n",
       "      <td>23.322453</td>\n",
       "      <td>6.860353</td>\n",
       "      <td>0.253994</td>\n",
       "      <td>0.115878</td>\n",
       "      <td>0.702617</td>\n",
       "      <td>28.148861</td>\n",
       "      <td>2.105710</td>\n",
       "      <td>8.707259</td>\n",
       "      <td>168.537116</td>\n",
       "      <td>...</td>\n",
       "      <td>157613.919181</td>\n",
       "      <td>3732.003358</td>\n",
       "      <td>64619.91997</td>\n",
       "      <td>5083.126041</td>\n",
       "      <td>76.318094</td>\n",
       "      <td>1837.557901</td>\n",
       "      <td>145.807763</td>\n",
       "      <td>41830.850997</td>\n",
       "      <td>2613.501822</td>\n",
       "      <td>236.061920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.006320</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.460000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.385000</td>\n",
       "      <td>3.561000</td>\n",
       "      <td>2.900000</td>\n",
       "      <td>1.129600</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>187.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>34969.000000</td>\n",
       "      <td>2692.800000</td>\n",
       "      <td>213.12000</td>\n",
       "      <td>498.960000</td>\n",
       "      <td>158.760000</td>\n",
       "      <td>6.464000</td>\n",
       "      <td>25.431000</td>\n",
       "      <td>0.102400</td>\n",
       "      <td>5.580800</td>\n",
       "      <td>2.992900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.082045</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.190000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.449000</td>\n",
       "      <td>5.885500</td>\n",
       "      <td>45.025000</td>\n",
       "      <td>2.100175</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>279.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>77841.000000</td>\n",
       "      <td>4913.600000</td>\n",
       "      <td>101240.04000</td>\n",
       "      <td>2051.340000</td>\n",
       "      <td>302.760000</td>\n",
       "      <td>6140.250000</td>\n",
       "      <td>122.942000</td>\n",
       "      <td>140908.274275</td>\n",
       "      <td>2355.214275</td>\n",
       "      <td>48.303700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.256510</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.690000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.538000</td>\n",
       "      <td>6.208500</td>\n",
       "      <td>77.500000</td>\n",
       "      <td>3.207450</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>330.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>108900.000000</td>\n",
       "      <td>5924.100000</td>\n",
       "      <td>119658.91500</td>\n",
       "      <td>3724.380000</td>\n",
       "      <td>362.905000</td>\n",
       "      <td>7078.025500</td>\n",
       "      <td>205.030000</td>\n",
       "      <td>153225.273700</td>\n",
       "      <td>3804.722600</td>\n",
       "      <td>129.050000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.677083</td>\n",
       "      <td>12.500000</td>\n",
       "      <td>18.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.624000</td>\n",
       "      <td>6.623500</td>\n",
       "      <td>94.075000</td>\n",
       "      <td>5.188425</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>666.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>443556.000000</td>\n",
       "      <td>13453.200000</td>\n",
       "      <td>160982.30250</td>\n",
       "      <td>8489.782500</td>\n",
       "      <td>408.040000</td>\n",
       "      <td>7695.286500</td>\n",
       "      <td>334.360500</td>\n",
       "      <td>156994.250700</td>\n",
       "      <td>5640.628650</td>\n",
       "      <td>287.472100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>88.976200</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>27.740000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.871000</td>\n",
       "      <td>8.780000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>12.126500</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>711.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>505521.000000</td>\n",
       "      <td>14291.100000</td>\n",
       "      <td>282195.90000</td>\n",
       "      <td>25288.020000</td>\n",
       "      <td>484.000000</td>\n",
       "      <td>8421.600000</td>\n",
       "      <td>766.994000</td>\n",
       "      <td>157529.610000</td>\n",
       "      <td>15070.293000</td>\n",
       "      <td>1441.720900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 104 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             CRIM          ZN       INDUS        CHAS         NOX          RM  \\\n",
       "count  506.000000  506.000000  506.000000  506.000000  506.000000  506.000000   \n",
       "mean     3.613524   11.363636   11.136779    0.069170    0.554695    6.284634   \n",
       "std      8.601545   23.322453    6.860353    0.253994    0.115878    0.702617   \n",
       "min      0.006320    0.000000    0.460000    0.000000    0.385000    3.561000   \n",
       "25%      0.082045    0.000000    5.190000    0.000000    0.449000    5.885500   \n",
       "50%      0.256510    0.000000    9.690000    0.000000    0.538000    6.208500   \n",
       "75%      3.677083   12.500000   18.100000    0.000000    0.624000    6.623500   \n",
       "max     88.976200  100.000000   27.740000    1.000000    0.871000    8.780000   \n",
       "\n",
       "              AGE         DIS         RAD         TAX  ...        TAX_TAX  \\\n",
       "count  506.000000  506.000000  506.000000  506.000000  ...     506.000000   \n",
       "mean    68.574901    3.795043    9.549407  408.237154  ...  195006.197628   \n",
       "std     28.148861    2.105710    8.707259  168.537116  ...  157613.919181   \n",
       "min      2.900000    1.129600    1.000000  187.000000  ...   34969.000000   \n",
       "25%     45.025000    2.100175    4.000000  279.000000  ...   77841.000000   \n",
       "50%     77.500000    3.207450    5.000000  330.000000  ...  108900.000000   \n",
       "75%     94.075000    5.188425   24.000000  666.000000  ...  443556.000000   \n",
       "max    100.000000   12.126500   24.000000  711.000000  ...  505521.000000   \n",
       "\n",
       "        TAX_PTRATIO         TAX_B     TAX_LSTAT  PTRATIO_PTRATIO    PTRATIO_B  \\\n",
       "count    506.000000     506.00000    506.000000       506.000000   506.000000   \n",
       "mean    7702.055336  138823.11502   5818.871146       345.284447  6547.619334   \n",
       "std     3732.003358   64619.91997   5083.126041        76.318094  1837.557901   \n",
       "min     2692.800000     213.12000    498.960000       158.760000     6.464000   \n",
       "25%     4913.600000  101240.04000   2051.340000       302.760000  6140.250000   \n",
       "50%     5924.100000  119658.91500   3724.380000       362.905000  7078.025500   \n",
       "75%    13453.200000  160982.30250   8489.782500       408.040000  7695.286500   \n",
       "max    14291.100000  282195.90000  25288.020000       484.000000  8421.600000   \n",
       "\n",
       "       PTRATIO_LSTAT            B_B       B_LSTAT  LSTAT_LSTAT  \n",
       "count     506.000000     506.000000    506.000000   506.000000  \n",
       "mean      239.290334  135534.645253   4274.823238   210.993989  \n",
       "std       145.807763   41830.850997   2613.501822   236.061920  \n",
       "min        25.431000       0.102400      5.580800     2.992900  \n",
       "25%       122.942000  140908.274275   2355.214275    48.303700  \n",
       "50%       205.030000  153225.273700   3804.722600   129.050000  \n",
       "75%       334.360500  156994.250700   5640.628650   287.472100  \n",
       "max       766.994000  157529.610000  15070.293000  1441.720900  \n",
       "\n",
       "[8 rows x 104 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adding all 2 degree features to the dataset\n",
    "for i in range(13):\n",
    "    for j in range(i, 13):\n",
    "        ele = df.columns[i] + \"_\" + df.columns[j]\n",
    "        df[ele] = df[df.columns[i]]*df[df.columns[j]]\n",
    "\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e05dccb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the input features (numpy array)\n",
    "X = df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7ff6996f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(506, 104)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "37dc3b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into test and train data (randomly without any bias)\n",
    "from sklearn import model_selection\n",
    "X_train, X_test, Y_train, Y_test = model_selection.train_test_split(X, Y, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e2999d3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(379, 104)\n",
      "(127, 104)\n",
      "(379,)\n",
      "(127,)\n"
     ]
    }
   ],
   "source": [
    "# Checking the shape of the data (to check the percentage of data taken to train and test the algorithm)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_train.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "88ae9643",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating the algorithm object that we can use to train and then test the data\n",
    "alg2 = LinearRegression()\n",
    "\n",
    "# Fitting/training the data\n",
    "alg2.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a32cd129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Score : 0.9520519609032727\n",
      "Test Score : 0.6074721959722745\n"
     ]
    }
   ],
   "source": [
    "# Predicting the output\n",
    "Y_pred = alg2.predict(X_test)\n",
    "\n",
    "# Finding the Train Score\n",
    "train_score = alg2.score(X_train, Y_train)\n",
    "\n",
    "# Finding the Test Score\n",
    "test_score = alg2.score(X_test, Y_test)\n",
    "\n",
    "print(\"Train Score :\", train_score)\n",
    "print(\"Test Score :\", test_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6227299",
   "metadata": {},
   "source": [
    "Optimization is a big part of machine learning. Almost every machine learning algorithm has an optimization algorithm at it’s core. Lets look at a very important yet simple optimization algorithm that you can use with any machine learning algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47378e9e",
   "metadata": {},
   "source": [
    "## **Gradient Descent**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4419f0",
   "metadata": {},
   "source": [
    "Gradient descent is an optimization algorithm used to find the values of parameters (coefficients) of a function (f) that minimizes a cost function (cost).\n",
    "\n",
    "Gradient descent is best used when the parameters cannot be calculated analytically (e.g. using linear algebra) and must be searched for by an optimization algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae15569d",
   "metadata": {},
   "source": [
    "#### **Intuition of Gradient Descent**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad1f0d2",
   "metadata": {},
   "source": [
    "Think of a large bowl like what you would eat cereal out of or store fruit in. This bowl is a plot of the cost function (f) in 3D space.\n",
    "\n",
    "A random position on the surface of the bowl is the cost of the current values of the coefficients (cost). The bottom of the bowl is the cost of the best set of coefficients, the minimum of the function.\n",
    "\n",
    "The goal is to continue to try different values for the coefficients, evaluate their cost and select new coefficients that have a slightly better (lower) cost.\n",
    "Repeating this process enough times will lead to the bottom of the bowl and you will know the values of the coefficients that result in the minimum cost."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aacb5a98",
   "metadata": {},
   "source": [
    "So our aim is to reach the line \n",
    "$$ y^p = mx + c $$, \n",
    "such that cost function\n",
    "$$ cost = \\sum_i (y_i - (mx_i + c))^2 $$\n",
    "is minimised.\n",
    "Here, m are the coefficents of the features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6bf7bd",
   "metadata": {},
   "source": [
    "The graph of our cost function will look like this :\n",
    "\n",
    "<img src = \"\thttps://files.codingninjas.in/graph_gd2-7053.jpg\" width = \"400\">\n",
    "\n",
    "where, $Cost_{min}$ and $m_{min}$ are the minimum values of $Cost$ and $m$ respectively.\n",
    "\n",
    "The idea is to select $m$ and $cost$ randomly in the beginning. Then, we will find the slope.\n",
    "\n",
    "If the slope is positive, the selected $m$ is to the right of $m_{min}$.\n",
    "\n",
    "If the slope is negative, the selected $m$ is to the left of $m_{min}$.\n",
    "\n",
    "Using the slope, the new optimised value of m ($m'$) can be calculated by :\n",
    "$$ m' = m - \\alpha(slope_m) $$\n",
    "\n",
    "Also, optimised intercept $c$ can be calculated by :\n",
    "$$ c' = c - \\alpha(slope_c) $$\n",
    "\n",
    "where,\n",
    "$\\alpha$ is the learning rate. \n",
    "$$ slope_m = \\frac{\\partial Cost}{\\partial m} $$\n",
    "and \n",
    "$$ slope_c = \\frac{\\partial Cost}{\\partial c} $$\n",
    "\n",
    "It is very easy to find the above two partials.\n",
    "Taking the derivative of $Cost$ wrt $m$ gives us\n",
    "\n",
    "$$ \\frac{\\partial Cost}{\\partial m} = \\frac{-2}{N}\\sum_i(y_i - mx_i - c)x_i $$\n",
    "Taking the derivative of $Cost$ wrt $c$ gives us\n",
    "\n",
    "$$ \\frac{\\partial Cost}{\\partial c} = \\frac{-2}{N}\\sum_i(y_i - mx_i - c) $$\n",
    "\n",
    "\n",
    "Now the question arises, how many times do we optimise $m$ and $c$?\n",
    "\n",
    "For each new value of $m$ and $c$, calculate the $Cost$ too. If all is done correclty, you will notice that with each new optimised value, optimised $Cost$ will keep decreasing.\n",
    "\n",
    "So, we keep on optimising $m$ and $c$ till we reach a point, where the change is $Cost$ (ie, the decrease in cost) is very less.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9e57de",
   "metadata": {},
   "source": [
    "#### **Learning Rate ($\\alpha$) and its Importance**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8992a26",
   "metadata": {},
   "source": [
    "How big the steps are gradient descent takes into the direction of the local minimum are determined by the learning rate, which figures out how fast or slow we will move towards the optimal weights.\n",
    "\n",
    "For gradient descent to reach the local minimum we must set the learning rate to an appropriate value, which is neither too low nor too high. This is important because if the steps it takes are too big, it may not reach the local minimum because it bounces back and forth between the convex function of gradient descent (see left image below). If we set the learning rate to a very small value, gradient descent will eventually reach the local minimum but that may take a while (see the right image). \n",
    "\n",
    "<img src=\"https://files.codingninjas.in/gradient-descent-learning-rate-7052.png\" width=\"550\">\n",
    "\n",
    "So, the learning rate should never be too high or too low for this reason. You can check if you’re learning rate is doing well by plotting it on a graph."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b89a87",
   "metadata": {},
   "source": [
    "#### **Adaptive Learning Rate**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05281588",
   "metadata": {},
   "source": [
    "The performance of the model on the training dataset can be monitored by the learning algorithm and the learning rate can be adjusted in response. \n",
    "This is called an adaptive learning rate. \n",
    "Perhaps the simplest implementation is to make the learning rate smaller once the performance of the model plateaus, such as by decreasing the learning rate by a factor of two.\n",
    "\n",
    "An adaptive learning rate method will generally outperform a model with a badly configured learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008df777",
   "metadata": {},
   "source": [
    "### **Let's code Gradient Descent for a Single Feature**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4fff5d",
   "metadata": {},
   "source": [
    "**Loading the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f9c2cccb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 2)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "data = np.loadtxt(\"https://files.codingninjas.in/data-6984.csv\", delimiter=\",\")\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8e88b2b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70, 2)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data = data[:70,]\n",
    "training_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ba10e986",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 2)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_data = data[70:,]\n",
    "testing_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931df9a0",
   "metadata": {},
   "source": [
    "**Now, using gradient descent, we will find the best values of m and c**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "248bcef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function finds the new gradient at each step\n",
    "def step_gradient(points, learning_rate, m , c):\n",
    "    m_slope = 0\n",
    "    c_slope = 0\n",
    "    M = len(points)\n",
    "    for i in range(M):\n",
    "        x = points[i, 0]\n",
    "        y = points[i, 1]\n",
    "        m_slope += (-2/M)* (y - m * x - c)*x\n",
    "        c_slope += (-2/M)* (y - m * x - c)\n",
    "    new_m = m - learning_rate * m_slope\n",
    "    new_c = c - learning_rate * c_slope\n",
    "    return new_m, new_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "243405a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Gradient Descent Function\n",
    "def gd(points, learning_rate, num_iterations):\n",
    "    m = 0       # Intial random value taken as 0\n",
    "    c = 0       # Intial random value taken as 0\n",
    "    for i in range(num_iterations):\n",
    "        m, c = step_gradient(points, learning_rate, m , c)\n",
    "        print(i, \" Cost: \", cost(points, m, c))\n",
    "    return m, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c3e65797",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function finds the new cost after each optimisation.\n",
    "def cost(points, m, c):\n",
    "    total_cost = 0\n",
    "    M = len(points)\n",
    "    for i in range(M):\n",
    "        x = points[i, 0]\n",
    "        y = points[i, 1]\n",
    "        total_cost += (1/M)*((y - m*x - c)**2)\n",
    "    return total_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0ab46b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run():\n",
    "    learning_rate = 0.0001\n",
    "    num_iterations = 100\n",
    "    m, c = gd(training_data, learning_rate, num_iterations)\n",
    "    print(\"Final m :\", m)\n",
    "    print(\"Final c :\", c)\n",
    "    return m,c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "92f8570f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  Cost:  1461.4044104341087\n",
      "1  Cost:  460.8670268567474\n",
      "2  Cost:  205.4870778024464\n",
      "3  Cost:  140.30318108579826\n",
      "4  Cost:  123.66545280139864\n",
      "5  Cost:  119.41878332450108\n",
      "6  Cost:  118.33484209854512\n",
      "7  Cost:  118.05816441204072\n",
      "8  Cost:  117.98753491264765\n",
      "9  Cost:  117.96949772470519\n",
      "10  Cost:  117.96488434447647\n",
      "11  Cost:  117.96369729432573\n",
      "12  Cost:  117.96338479030575\n",
      "13  Cost:  117.96329550799736\n",
      "14  Cost:  117.9632632015475\n",
      "15  Cost:  117.9632454379033\n",
      "16  Cost:  117.96323138633423\n",
      "17  Cost:  117.96321828237488\n",
      "18  Cost:  117.96320542041524\n",
      "19  Cost:  117.96319262035338\n",
      "20  Cost:  117.9631798362198\n",
      "21  Cost:  117.96316705628094\n",
      "22  Cost:  117.96315427754192\n",
      "23  Cost:  117.96314149923846\n",
      "24  Cost:  117.96312872117527\n",
      "25  Cost:  117.96311594330255\n",
      "26  Cost:  117.9631031656078\n",
      "27  Cost:  117.9630903880875\n",
      "28  Cost:  117.96307761074107\n",
      "29  Cost:  117.96306483356811\n",
      "30  Cost:  117.96305205656859\n",
      "31  Cost:  117.96303927974257\n",
      "32  Cost:  117.96302650309\n",
      "33  Cost:  117.96301372661085\n",
      "34  Cost:  117.96300095030523\n",
      "35  Cost:  117.96298817417302\n",
      "36  Cost:  117.96297539821425\n",
      "37  Cost:  117.96296262242893\n",
      "38  Cost:  117.96294984681701\n",
      "39  Cost:  117.96293707137862\n",
      "40  Cost:  117.96292429611363\n",
      "41  Cost:  117.962911521022\n",
      "42  Cost:  117.96289874610383\n",
      "43  Cost:  117.96288597135916\n",
      "44  Cost:  117.96287319678783\n",
      "45  Cost:  117.96286042238998\n",
      "46  Cost:  117.96284764816549\n",
      "47  Cost:  117.9628348741145\n",
      "48  Cost:  117.96282210023688\n",
      "49  Cost:  117.9628093265326\n",
      "50  Cost:  117.96279655300184\n",
      "51  Cost:  117.96278377964448\n",
      "52  Cost:  117.96277100646049\n",
      "53  Cost:  117.96275823344999\n",
      "54  Cost:  117.96274546061275\n",
      "55  Cost:  117.96273268794901\n",
      "56  Cost:  117.96271991545866\n",
      "57  Cost:  117.96270714314169\n",
      "58  Cost:  117.96269437099812\n",
      "59  Cost:  117.9626815990279\n",
      "60  Cost:  117.96266882723108\n",
      "61  Cost:  117.96265605560774\n",
      "62  Cost:  117.9626432841577\n",
      "63  Cost:  117.96263051288103\n",
      "64  Cost:  117.9626177417778\n",
      "65  Cost:  117.9626049708479\n",
      "66  Cost:  117.96259220009142\n",
      "67  Cost:  117.96257942950821\n",
      "68  Cost:  117.96256665909843\n",
      "69  Cost:  117.96255388886206\n",
      "70  Cost:  117.96254111879902\n",
      "71  Cost:  117.96252834890933\n",
      "72  Cost:  117.962515579193\n",
      "73  Cost:  117.96250280965003\n",
      "74  Cost:  117.96249004028041\n",
      "75  Cost:  117.96247727108418\n",
      "76  Cost:  117.96246450206127\n",
      "77  Cost:  117.96245173321168\n",
      "78  Cost:  117.96243896453551\n",
      "79  Cost:  117.9624261960326\n",
      "80  Cost:  117.96241342770308\n",
      "81  Cost:  117.96240065954687\n",
      "82  Cost:  117.96238789156405\n",
      "83  Cost:  117.96237512375453\n",
      "84  Cost:  117.9623623561184\n",
      "85  Cost:  117.9623495886555\n",
      "86  Cost:  117.96233682136597\n",
      "87  Cost:  117.96232405424978\n",
      "88  Cost:  117.96231128730685\n",
      "89  Cost:  117.96229852053732\n",
      "90  Cost:  117.96228575394112\n",
      "91  Cost:  117.96227298751815\n",
      "92  Cost:  117.96226022126854\n",
      "93  Cost:  117.96224745519224\n",
      "94  Cost:  117.96223468928922\n",
      "95  Cost:  117.96222192355955\n",
      "96  Cost:  117.96220915800318\n",
      "97  Cost:  117.96219639262009\n",
      "98  Cost:  117.96218362741031\n",
      "99  Cost:  117.96217086237382\n",
      "Final m : 1.458255777804894\n",
      "Final c : 0.032397159787702676\n"
     ]
    }
   ],
   "source": [
    "m, c = run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155b8648",
   "metadata": {},
   "source": [
    "It can be seen, that the cost for the last many iterations remains almost the same, which is 117.962.\n",
    "\n",
    "For this cost, the final m is found to be 1.4582 and final c is found to be 0.0323.\n",
    "\n",
    "These optimised values may then be plugged into our hypothesis funcion to find $y_{pred}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16b9251",
   "metadata": {},
   "source": [
    "**Predictions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c67717a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(final_m, final_c, testing_data):\n",
    "    y_pred = []\n",
    "    for i in range(len(testing_data)):\n",
    "        ans = m*testing_data[i][0] + c\n",
    "        y_pred.append(ans)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8ded9122",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[46.095951282291885,\n",
       " 78.28376167276944,\n",
       " 68.10702680868928,\n",
       " 62.8946250628685,\n",
       " 102.61496837133335,\n",
       " 64.91436131908361,\n",
       " 83.8887150992528,\n",
       " 53.885894749919025,\n",
       " 81.41143026364702,\n",
       " 56.838414234095204,\n",
       " 83.00892226345628,\n",
       " 82.96180012666355,\n",
       " 50.09887462980676,\n",
       " 86.14202346395936,\n",
       " 84.30240868699974,\n",
       " 79.18991662796913,\n",
       " 74.5328181331299,\n",
       " 73.35763378901311,\n",
       " 64.50442501659097,\n",
       " 55.45411963583681,\n",
       " 48.06804235977706,\n",
       " 78.32854078411849,\n",
       " 100.31042647345949,\n",
       " 67.44897116945312,\n",
       " 99.65949980894587,\n",
       " 72.98918795613403,\n",
       " 71.83656946861555,\n",
       " 73.00289789301861,\n",
       " 70.24720708812089,\n",
       " 36.67615508488324]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = predict(m, c, testing_data)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769cedbe",
   "metadata": {},
   "source": [
    "### **Using the inbuilt Gradient Booster**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5e63e88d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor()"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "model = GradientBoostingRegressor()\n",
    "x_train = training_data[:,0].reshape(-1, 1)\n",
    "y_train = training_data[:,1]\n",
    "model.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "afe5ef67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([33.05621077, 82.80773117, 73.60871863, 62.80258768, 85.242871  ,\n",
       "       62.80258768, 77.29833337, 60.4283958 , 83.40209375, 58.3415076 ,\n",
       "       83.40209375, 83.40209375, 51.3317227 , 77.88979793, 77.29833337,\n",
       "       82.80773117, 75.26029188, 72.23701306, 62.80258768, 76.2986538 ,\n",
       "       51.19104556, 82.80773117, 85.242871  , 73.60871863, 85.242871  ,\n",
       "       78.9358657 , 79.11953733, 78.9358657 , 68.91225498, 33.05621077])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict(testing_data[:,0].reshape(-1, 1))\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f15ae6",
   "metadata": {},
   "source": [
    "#### **Types of Gradient Descent**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b793d41",
   "metadata": {},
   "source": [
    "***Batch Gradient Descent***\n",
    "\n",
    "Batch gradient descent, also called vanilla gradient descent, calculates the error for each example within the training dataset, but only after all training examples have been evaluated does the model get updated. This whole process is like a cycle and it's called a training epoch.\n",
    "\n",
    "Some advantages of batch gradient descent are that it's computationally efficient, it produces a stable error gradient and a stable convergence. Some disadvantages are the stable error gradient can sometimes result in a state of convergence that isn’t the best the model can achieve. It also requires the entire training dataset be in memory and available to the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fadf326",
   "metadata": {},
   "source": [
    "***Stochastic Gradient Descent***\n",
    "\n",
    "By contrast, stochastic gradient descent (SGD) does this for each training example within the dataset, meaning it updates the parameters for each training example one by one. Depending on the problem, this can make SGD faster than batch gradient descent. One advantage is the frequent updates allow us to have a pretty detailed rate of improvement.\n",
    "\n",
    "The frequent updates, however, are more computationally expensive than the batch gradient descent approach. Additionally, the frequency of those updates can result in noisy gradients, which may cause the error rate to jump around instead of slowly decreasing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95cbf90",
   "metadata": {},
   "source": [
    "***Mini-Batch Gradient Descent***\n",
    "\n",
    "Mini-batch gradient descent is the go-to method since it’s a combination of the concepts of SGD and batch gradient descent. It simply splits the training dataset into small batches and performs an update for each of those batches. This creates a balance between the robustness of stochastic gradient descent and the efficiency of batch gradient descent.\n",
    "\n",
    "Common mini-batch sizes range between 50 and 256, but like any other machine learning technique, there is no clear rule because it varies for different applications. This is the go-to algorithm when training a neural network and it is the most common type of gradient descent within deep learning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
